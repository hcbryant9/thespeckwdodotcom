<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EmotionAnalysis</title>
    
    <style>
        #image1 {
            width: 350px;
            height: 250px;
        }
        
        #quote {
            width: 50%; /* Adjust the width as needed */
            margin-left: 3%; /* Center the quote */
        }

        #image2 {
            width: 300px;
            height: 30px;
        }
        #image3{
            width: 350px;
            height: 200px;
        }
        p{
            width:60%
        }
    </style>
    
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QQ6JMSNG3C"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-QQ6JMSNG3C');
</script>
<body>
    <nav>
        <a href="index.html">Home</a>
        <a href="projects.html">Projects</a>
    </nav>

    <br>
    <br>
    <p1>emotional-surveillance-ml</p1>
    <p><small>This page is dedicated to document my personal work on the larger project <i>Sentiment Voice Analysis</i>, a Virtual Reality experience that integrates user emotions into an adaptive environment, aimed to shed light on emotional data tracking for public awareness and engagement. The project is a collaborative effort among several engineers and artists.</small> </p>
    
    <img id="image1" src="assets/images/emotions/dynamicfacialexpressions3.png" alt="facialexpression"><br>
    <br>
    <a href="https://github.com/hcbryant9/EmotionAnalysis">Project GitHub</a>
    <br>
    <p><i>02/01/2024 update</i><br><br>
        Today I collected data for 4 emotions [happy,sad,mad,anxious]. Currently, the Unity project runs through each emotion 5 times for a total of 24 data points. I did this process twice on my face so I had 48 data points. I trained the model with 47 data points leaving out 1 data point for testing.
        After training the model with these points here were the results after running emotionclassifier.py.
        <br>
        <img id = "image3" src="assets/images/emotions/emotions-photo-2.png" alt="emotion-photo-2">
        <br>
        Then taking the models and making a prediction on the data point I left out I recorded this after running prediction.py.
        <br>
        <img id = "image2" src="assets/images/emotions/emotions-photo-3.png" alt="emotion-photo-3">
        <br>
        Both random forest and decision tree models correctly predicted the 'anxious' expression.
    </p>
    <br>
    <p>Working on this project has created an inner dialouge about the societal impact of software development. These thoughts have manifested themself into spreading awareness within my community and building an emotion predicting model.
        Currently, I am collecting facial tracking data from the Meta Quest Pro to create a machine learning model that will predict the emotionof the user. The GitHub link above has the python code needed to generate the model. 
        All that is needed is the data.
    </p>
    <br>
    <blockquote id="quote">
        <i> "And while there is an enormous structural power asymmetry between the surveillers and surveilled, neither are those with the greatest power free from being haunted by a very particular kind of data anxiety: that no matter how much data they have, it is always incomplete, and the sheer volume can overwhelm the critical signals in a fog of possible correlations."</i>
        <br><cite><a href="https://thenewinquiry.com/the-anxieties-of-big-data/">Kate Crawford</a></cite>
    </blockquote>
</body>
</html>
